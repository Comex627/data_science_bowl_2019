{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LigthGBM - CLASSIFICATION - BAYESIAN OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn import datasets\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/train_test/'\n",
    "SEED = 47\n",
    "NITER = 100\n",
    "CV = 3\n",
    "SCORE = 'roc_auc'\n",
    "handlingnull = False\n",
    "NJOBS = -1\n",
    "USEGPU = False\n",
    "NCLASS = 3 # number class to predict (if bivar set 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_pickle(DATAPATH+'X_train.pkl').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_pickle(DATAPATH+'y_train.pkl')['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148865, 1770)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148865,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if handlingnull:\n",
    "    train_features[np.isnan(train_features)] = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Search hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== General Parameters ======= #\n",
    "\n",
    "# Select the type of model to run at each iteration. gbtree or gblinear.\n",
    "boosting = 'gbdt'\n",
    "\n",
    "\n",
    "# ======== Booster Parameters ======== # \n",
    "\n",
    "# Analogous to learning rate in GBM. \n",
    "# Typical final values to be used: 0.01-0.2\n",
    "eta = [0.01] \n",
    "\n",
    "\n",
    "# A node is split only when the resulting split gives a positive reduction in the loss function. \n",
    "# Gamma specifies the minimum loss reduction required to make a split.\n",
    "gamma = [i/10.0 for i in range(0,5)]\n",
    "\n",
    "\n",
    "# Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "# A typical value to consider: sum(negative instances) / sum(positive instances)scale_pos_weight = 1\n",
    "scale_pos_weight = (len(train_labels) - sum(train_labels))/sum(train_labels)\n",
    "\n",
    "\n",
    "# Learning Task Parameters\n",
    "# This defines the loss function to be minimized. See documentation\n",
    "# -  options: regression, regression_l1, huber, fair, poisson, quantile, \n",
    "# mape, gamma, tweedie, binary, multiclass, multiclassova, cross_entropy, cross_entropy_lambda,\n",
    "# lambdarank, aliases: objective_type, app, application\n",
    "objective  = 'binary'\n",
    "\n",
    "\n",
    "# The metric to be used for validation data.\n",
    "# - rmse, root square loss, aliases: root_mean_squared_error, l2_root\n",
    "# - quantile, Quantile regression\n",
    "# - mape, MAPE loss, aliases: mean_absolute_percentage_error\n",
    "# - huber, Huber loss\n",
    "# - fair, Fair loss\n",
    "# - poisson, negative log-likelihood for Poisson regression\n",
    "# - gamma, negative log-likelihood for Gamma regression\n",
    "# - gamma_deviance, residual deviance for Gamma regression\n",
    "# - tweedie, negative log-likelihood for Tweedie regression\n",
    "# - ndcg, NDCG, aliases: lambdarank\n",
    "# - map, MAP, aliases: mean_average_precision\n",
    "# - auc, AUC\n",
    "# - binary_logloss, log loss, aliases: binary\n",
    "metric = 'auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[lightGBM params](https://lightgbm.readthedocs.io/en/latest/Parameters.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "help(lgb.LGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "# Domain space-- Range of hyperparameters\n",
    "pds = {\n",
    "    # Minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "    # default = 20, type = int, aliases: min_data_per_leaf, min_data, min_child_samples, \n",
    "    'num_leaves': (20, 100),\n",
    "\n",
    "    # Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    'feature_fraction': (0.1, 0.9),\n",
    "    \n",
    "    # Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    'bagging_fraction': (0.8, 1),\n",
    "\n",
    "    # The maximum depth of a tree\n",
    "    'max_depth': (9, 13 ),\n",
    "\n",
    "    'min_split_gain': (0.001, 0.1),\n",
    "\n",
    "    # Minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "    # Default 1e-3\n",
    "    'min_child_weight': (30, 50),\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': (20, 100),\n",
       " 'feature_fraction': (0.1, 0.9),\n",
       " 'bagging_fraction': (0.8, 1),\n",
       " 'max_depth': (9, 13),\n",
       " 'min_split_gain': (0.001, 0.1),\n",
       " 'min_child_weight': (30, 50)}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find num boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your configurations as a dict\n",
    "lgb_params = {\n",
    "    'boosting_type': boosting,\n",
    "    'objective': objective,\n",
    "    'metric': metric,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'num_threads' : 8,\n",
    "    'verbose': 0,\n",
    "    #'num_class':  NCLASS,\n",
    "    'seed' : SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresult = lgb.cv(lgb_params, lgb_train, num_boost_round = 1000, nfold = CV, metrics = metric, early_stopping_rounds = early_stopping_rounds, seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = pd.DataFrame(cvresult).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of estimators found:  285\n"
     ]
    }
   ],
   "source": [
    "print(\"Best number of estimators found: \", n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight):\n",
    "      \n",
    "    params = {'boosting_type': boosting,\n",
    "              'application': objective,\n",
    "              'num_iterations': n_estimators,\n",
    "              'learning_rate':eta, \n",
    "              'early_stopping_round':50,\n",
    "              'metric': metric} # Default parameters\n",
    "    \n",
    "    \n",
    "    params[\"num_leaves\"] = int(round(num_leaves))\n",
    "    params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "    params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "    params['max_depth'] = int(round(max_depth))\n",
    "    params['min_split_gain'] = min_split_gain\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    params['min_split_gain'] = min_split_gain\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_result = lgb.cv(params, lgb_train, nfold=CV, seed=SEED, stratified=False, verbose_eval=None, metrics = metric)\n",
    "    \n",
    "    # Extract the best score\n",
    "    best_score = max(cv_result['auc-mean'])\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    #loss = 1 - best_score\n",
    "\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surrogate model\n",
    "optimizer = BayesianOptimization(hyp_lgbm,pds,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | max_depth | min_ch... | min_sp... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7949  \u001b[0m | \u001b[0m 0.8153  \u001b[0m | \u001b[0m 0.7239  \u001b[0m | \u001b[0m 10.75   \u001b[0m | \u001b[0m 44.47   \u001b[0m | \u001b[0m 0.09782 \u001b[0m | \u001b[0m 63.08   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.79    \u001b[0m | \u001b[0m 0.9002  \u001b[0m | \u001b[0m 0.1576  \u001b[0m | \u001b[0m 10.07   \u001b[0m | \u001b[0m 40.0    \u001b[0m | \u001b[0m 0.06824 \u001b[0m | \u001b[0m 84.3    \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7883  \u001b[0m | \u001b[0m 0.8762  \u001b[0m | \u001b[0m 0.1527  \u001b[0m | \u001b[0m 10.15   \u001b[0m | \u001b[0m 48.19   \u001b[0m | \u001b[0m 0.02213 \u001b[0m | \u001b[0m 56.17   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7845  \u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 0.1199  \u001b[0m | \u001b[0m 11.4    \u001b[0m | \u001b[0m 49.0    \u001b[0m | \u001b[0m 0.0238  \u001b[0m | \u001b[0m 63.88   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7909  \u001b[0m | \u001b[0m 0.9818  \u001b[0m | \u001b[0m 0.2065  \u001b[0m | \u001b[0m 11.09   \u001b[0m | \u001b[0m 45.01   \u001b[0m | \u001b[0m 0.06723 \u001b[0m | \u001b[0m 57.42   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7887  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 20.0    \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.782   \u001b[0m | \u001b[0m 0.8     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 57.0    \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7885  \u001b[0m | \u001b[0m 0.9785  \u001b[0m | \u001b[0m 0.8507  \u001b[0m | \u001b[0m 12.54   \u001b[0m | \u001b[0m 49.17   \u001b[0m | \u001b[0m 0.03535 \u001b[0m | \u001b[0m 20.27   \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.7963  \u001b[0m | \u001b[95m 0.8146  \u001b[0m | \u001b[95m 0.8518  \u001b[0m | \u001b[95m 12.96   \u001b[0m | \u001b[95m 30.3    \u001b[0m | \u001b[95m 0.0838  \u001b[0m | \u001b[95m 99.65   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7952  \u001b[0m | \u001b[0m 0.8635  \u001b[0m | \u001b[0m 0.8725  \u001b[0m | \u001b[0m 11.45   \u001b[0m | \u001b[0m 49.93   \u001b[0m | \u001b[0m 0.03626 \u001b[0m | \u001b[0m 99.88   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7946  \u001b[0m | \u001b[0m 0.9426  \u001b[0m | \u001b[0m 0.7969  \u001b[0m | \u001b[0m 9.117   \u001b[0m | \u001b[0m 37.47   \u001b[0m | \u001b[0m 0.05733 \u001b[0m | \u001b[0m 99.85   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7925  \u001b[0m | \u001b[0m 0.8041  \u001b[0m | \u001b[0m 0.8556  \u001b[0m | \u001b[0m 12.94   \u001b[0m | \u001b[0m 37.14   \u001b[0m | \u001b[0m 0.0237  \u001b[0m | \u001b[0m 34.97   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7918  \u001b[0m | \u001b[0m 0.9013  \u001b[0m | \u001b[0m 0.8985  \u001b[0m | \u001b[0m 9.023   \u001b[0m | \u001b[0m 44.96   \u001b[0m | \u001b[0m 0.025   \u001b[0m | \u001b[0m 34.94   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.7959  \u001b[0m | \u001b[0m 0.8405  \u001b[0m | \u001b[0m 0.8772  \u001b[0m | \u001b[0m 12.92   \u001b[0m | \u001b[0m 38.95   \u001b[0m | \u001b[0m 0.09229 \u001b[0m | \u001b[0m 99.91   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7957  \u001b[0m | \u001b[0m 0.9875  \u001b[0m | \u001b[0m 0.8902  \u001b[0m | \u001b[0m 12.65   \u001b[0m | \u001b[0m 30.52   \u001b[0m | \u001b[0m 0.002858\u001b[0m | \u001b[0m 79.46   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.7944  \u001b[0m | \u001b[0m 0.8187  \u001b[0m | \u001b[0m 0.892   \u001b[0m | \u001b[0m 12.98   \u001b[0m | \u001b[0m 40.43   \u001b[0m | \u001b[0m 0.05246 \u001b[0m | \u001b[0m 55.24   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.7943  \u001b[0m | \u001b[0m 0.8207  \u001b[0m | \u001b[0m 0.8914  \u001b[0m | \u001b[0m 9.028   \u001b[0m | \u001b[0m 33.21   \u001b[0m | \u001b[0m 0.09803 \u001b[0m | \u001b[0m 91.07   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.7962  \u001b[0m | \u001b[0m 0.9895  \u001b[0m | \u001b[0m 0.898   \u001b[0m | \u001b[0m 12.73   \u001b[0m | \u001b[0m 35.41   \u001b[0m | \u001b[0m 0.09331 \u001b[0m | \u001b[0m 99.62   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.7955  \u001b[0m | \u001b[0m 0.8089  \u001b[0m | \u001b[0m 0.8923  \u001b[0m | \u001b[0m 12.71   \u001b[0m | \u001b[0m 39.32   \u001b[0m | \u001b[0m 0.008968\u001b[0m | \u001b[0m 77.76   \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Optimize\n",
    "optimizer.maximize(init_points=5, n_iter=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8145882315796769,\n",
       " 'feature_fraction': 0.8517987893796524,\n",
       " 'max_depth': 12.957248699729796,\n",
       " 'min_child_weight': 30.301446391267557,\n",
       " 'min_split_gain': 0.08380172857159027,\n",
       " 'num_leaves': 99.64679476791302}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('output/models/bayesianoptcv_gbm_classifier_bestparams_d' + str(datetime.now().date()) + '.npy', optimizer.max['params'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xgboost (env)",
   "language": "python",
   "name": "xgboostenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
